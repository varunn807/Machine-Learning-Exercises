{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part 1: Polynomial Regression\n",
    "\n",
    "### A) Use the Auto dataset, find the test $R^2$ score of a linear regression model that predicts the miles per gallon (mpg) from the horsepower.\n",
    "\n",
    "### B) Use polynomial regression to include both the horsepower feature and $(horsepower)^2$ in the regression model. Find the $R^2$ metric. \n",
    "\n",
    "Hint: You can use [numpy.concatenate](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.concatenate.html). For example to add to an array U a column vector $W^2$, we can use X=np.concatenate((U,W**2),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "      mpg  cylinders  displacement  horsepower  weight  acceleration  year  \\\n",
      "0    18.0          8         307.0         130    3504          12.0    70   \n",
      "1    15.0          8         350.0         165    3693          11.5    70   \n",
      "2    18.0          8         318.0         150    3436          11.0    70   \n",
      "3    16.0          8         304.0         150    3433          12.0    70   \n",
      "4    17.0          8         302.0         140    3449          10.5    70   \n",
      "5    15.0          8         429.0         198    4341          10.0    70   \n",
      "6    14.0          8         454.0         220    4354           9.0    70   \n",
      "7    14.0          8         440.0         215    4312           8.5    70   \n",
      "8    14.0          8         455.0         225    4425          10.0    70   \n",
      "9    15.0          8         390.0         190    3850           8.5    70   \n",
      "10   15.0          8         383.0         170    3563          10.0    70   \n",
      "11   14.0          8         340.0         160    3609           8.0    70   \n",
      "12   15.0          8         400.0         150    3761           9.5    70   \n",
      "13   14.0          8         455.0         225    3086          10.0    70   \n",
      "14   24.0          4         113.0          95    2372          15.0    70   \n",
      "15   22.0          6         198.0          95    2833          15.5    70   \n",
      "16   18.0          6         199.0          97    2774          15.5    70   \n",
      "17   21.0          6         200.0          85    2587          16.0    70   \n",
      "18   27.0          4          97.0          88    2130          14.5    70   \n",
      "19   26.0          4          97.0          46    1835          20.5    70   \n",
      "20   25.0          4         110.0          87    2672          17.5    70   \n",
      "21   24.0          4         107.0          90    2430          14.5    70   \n",
      "22   25.0          4         104.0          95    2375          17.5    70   \n",
      "23   26.0          4         121.0         113    2234          12.5    70   \n",
      "24   21.0          6         199.0          90    2648          15.0    70   \n",
      "25   10.0          8         360.0         215    4615          14.0    70   \n",
      "26   10.0          8         307.0         200    4376          15.0    70   \n",
      "27   11.0          8         318.0         210    4382          13.5    70   \n",
      "28    9.0          8         304.0         193    4732          18.5    70   \n",
      "29   27.0          4          97.0          88    2130          14.5    71   \n",
      "..    ...        ...           ...         ...     ...           ...   ...   \n",
      "362  28.0          4         112.0          88    2605          19.6    82   \n",
      "363  27.0          4         112.0          88    2640          18.6    82   \n",
      "364  34.0          4         112.0          88    2395          18.0    82   \n",
      "365  31.0          4         112.0          85    2575          16.2    82   \n",
      "366  29.0          4         135.0          84    2525          16.0    82   \n",
      "367  27.0          4         151.0          90    2735          18.0    82   \n",
      "368  24.0          4         140.0          92    2865          16.4    82   \n",
      "369  36.0          4         105.0          74    1980          15.3    82   \n",
      "370  37.0          4          91.0          68    2025          18.2    82   \n",
      "371  31.0          4          91.0          68    1970          17.6    82   \n",
      "372  38.0          4         105.0          63    2125          14.7    82   \n",
      "373  36.0          4          98.0          70    2125          17.3    82   \n",
      "374  36.0          4         120.0          88    2160          14.5    82   \n",
      "375  36.0          4         107.0          75    2205          14.5    82   \n",
      "376  34.0          4         108.0          70    2245          16.9    82   \n",
      "377  38.0          4          91.0          67    1965          15.0    82   \n",
      "378  32.0          4          91.0          67    1965          15.7    82   \n",
      "379  38.0          4          91.0          67    1995          16.2    82   \n",
      "380  25.0          6         181.0         110    2945          16.4    82   \n",
      "381  38.0          6         262.0          85    3015          17.0    82   \n",
      "382  26.0          4         156.0          92    2585          14.5    82   \n",
      "383  22.0          6         232.0         112    2835          14.7    82   \n",
      "384  32.0          4         144.0          96    2665          13.9    82   \n",
      "385  36.0          4         135.0          84    2370          13.0    82   \n",
      "386  27.0          4         151.0          90    2950          17.3    82   \n",
      "387  27.0          4         140.0          86    2790          15.6    82   \n",
      "388  44.0          4          97.0          52    2130          24.6    82   \n",
      "389  32.0          4         135.0          84    2295          11.6    82   \n",
      "390  28.0          4         120.0          79    2625          18.6    82   \n",
      "391  31.0          4         119.0          82    2720          19.4    82   \n",
      "\n",
      "     origin                               name  \n",
      "0         1          chevrolet chevelle malibu  \n",
      "1         1                  buick skylark 320  \n",
      "2         1                 plymouth satellite  \n",
      "3         1                      amc rebel sst  \n",
      "4         1                        ford torino  \n",
      "5         1                   ford galaxie 500  \n",
      "6         1                   chevrolet impala  \n",
      "7         1                  plymouth fury iii  \n",
      "8         1                   pontiac catalina  \n",
      "9         1                 amc ambassador dpl  \n",
      "10        1                dodge challenger se  \n",
      "11        1                 plymouth 'cuda 340  \n",
      "12        1              chevrolet monte carlo  \n",
      "13        1            buick estate wagon (sw)  \n",
      "14        3              toyota corona mark ii  \n",
      "15        1                    plymouth duster  \n",
      "16        1                         amc hornet  \n",
      "17        1                      ford maverick  \n",
      "18        3                       datsun pl510  \n",
      "19        2       volkswagen 1131 deluxe sedan  \n",
      "20        2                        peugeot 504  \n",
      "21        2                        audi 100 ls  \n",
      "22        2                           saab 99e  \n",
      "23        2                           bmw 2002  \n",
      "24        1                        amc gremlin  \n",
      "25        1                          ford f250  \n",
      "26        1                          chevy c20  \n",
      "27        1                         dodge d200  \n",
      "28        1                           hi 1200d  \n",
      "29        3                       datsun pl510  \n",
      "..      ...                                ...  \n",
      "362       1                 chevrolet cavalier  \n",
      "363       1           chevrolet cavalier wagon  \n",
      "364       1          chevrolet cavalier 2-door  \n",
      "365       1         pontiac j2000 se hatchback  \n",
      "366       1                     dodge aries se  \n",
      "367       1                    pontiac phoenix  \n",
      "368       1               ford fairmont futura  \n",
      "369       2                volkswagen rabbit l  \n",
      "370       3                 mazda glc custom l  \n",
      "371       3                   mazda glc custom  \n",
      "372       1             plymouth horizon miser  \n",
      "373       1                     mercury lynx l  \n",
      "374       3                   nissan stanza xe  \n",
      "375       3                       honda accord  \n",
      "376       3                     toyota corolla  \n",
      "377       3                        honda civic  \n",
      "378       3                 honda civic (auto)  \n",
      "379       3                      datsun 310 gx  \n",
      "380       1              buick century limited  \n",
      "381       1  oldsmobile cutlass ciera (diesel)  \n",
      "382       1         chrysler lebaron medallion  \n",
      "383       1                     ford granada l  \n",
      "384       3                   toyota celica gt  \n",
      "385       1                  dodge charger 2.2  \n",
      "386       1                   chevrolet camaro  \n",
      "387       1                    ford mustang gl  \n",
      "388       2                          vw pickup  \n",
      "389       1                      dodge rampage  \n",
      "390       1                        ford ranger  \n",
      "391       1                         chevy s-10  \n",
      "\n",
      "[392 rows x 9 columns]\n",
      "The R2 score is : 0.6217658811398383\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from pandas import read_csv\n",
    "AutoData=read_csv('Auto_modify.csv') # read the data\n",
    "print(type(AutoData))\n",
    "print(AutoData)\n",
    "X_auto_hp=AutoData.horsepower.values.reshape(-1,1) # define features: horsepower \n",
    "Y_auto_mpg=AutoData.mpg.values.reshape(-1,1) # define label: miles per gallon\n",
    "\n",
    "# add your solution here\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X_auto_hp,Y_auto_mpg,random_state=0)\n",
    "linreg=LinearRegression().fit(X_train,Y_train)\n",
    "r2=linreg.score(X_test,Y_test)\n",
    "\n",
    "print(\"The R2 score is :\",r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R2 score is : 0.7271031504642004\n"
     ]
    }
   ],
   "source": [
    "AutoData['Polynomial(horsepower^2)']=AutoData.horsepower.values*AutoData.horsepower.values\n",
    "\n",
    "X_auto_polynomial=AutoData[['horsepower','Polynomial(horsepower^2)']].values # define features: horsepower ,horsepower^2\n",
    "Y_auto_mpg=AutoData.mpg.values.reshape(-1,1) # define label: miles per gallon\n",
    "\n",
    "# add your solution here\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X_auto_polynomial,Y_auto_mpg,random_state=0)\n",
    "linreg=LinearRegression().fit(X_train,Y_train)\n",
    "r2=linreg.score(X_test,Y_test)\n",
    "\n",
    "print(\"The R2 score is :\",r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that the R2 score changes significantly after including the polynomial term horsepower^2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C)Use KNN regression to predict the miles per gallon(mpg) with K=7, and find $R^2$ metric in the following cases \n",
    "\n",
    "- One feature: Horsepower only\n",
    "\n",
    "- Two features: horsepower and $(horsepower)^2$ \n",
    "\n",
    "Hint: \n",
    "\n",
    "    Create KNN regression object using neighbors.KNeighborsRegressor:\n",
    "\n",
    "    knnRegression = neighbors.KNeighborsRegressor(n_neighbors=7)\n",
    "\n",
    "    Use the .fit and .score methods as before\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R2 score is : 0.6674777441714226\n"
     ]
    }
   ],
   "source": [
    "from sklearn import neighbors\n",
    "# add your solution here\n",
    "knnRegression = neighbors.KNeighborsRegressor(n_neighbors=7)\n",
    "\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X_auto_hp,Y_auto_mpg,random_state=0)\n",
    "knnRegression.fit(X_train,Y_train)\n",
    "\n",
    "r2=knnRegression.score(X_test,Y_test)\n",
    "print(\"The R2 score is :\",r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R2 score is : 0.6701084048823853\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X_auto_polynomial,Y_auto_mpg,random_state=0)\n",
    "knnRegression.fit(X_train,Y_train)\n",
    "\n",
    "r2=knnRegression.score(X_test,Y_test)\n",
    "print(\"The R2 score is :\",r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#The Linear model with quadratic feature performs best amongst the four options.But if we don't include quadratic feature then KNN regressor works better.This may be since KNN works better when number of features are less.\n",
    "Safe to say that, whichever the case(KNN or Linear), performance increases with introducing the quadratic feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COMMENT on your results on (E) and (F): which model performs better? How does performance change when adding the quadratic feature?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part 2: Regularization\n",
    "\n",
    "### A) Use the Boston dataset, and use Ridge regression model with tuning parameter set to 100 (alpha =100). Find the $R^2$ score and number of non zero coefficients.\n",
    "\n",
    "###  B) Use Lasso regression instead of Ridge regression, also set the tuning parameter to 100. Find the $R^2$ score and number of non zero coefficients.\n",
    "\n",
    "### C) Change the tuning parameter of the Lasso model to a very low value (alpha =0.001). What is the $R^2$ score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston House Prices dataset\n",
      "===========================\n",
      "\n",
      "Notes\n",
      "------\n",
      "Data Set Characteristics:  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive\n",
      "    \n",
      "    :Median Value (attribute 14) is usually the target\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "http://archive.ics.uci.edu/ml/datasets/Housing\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      "**References**\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n",
      "\n",
      "The R2 score for Ridge is 0.5925358036157629\n",
      "The number of non-zero coefficients are : 13\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import Ridge \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge \n",
    "from sklearn.linear_model import Lasso\n",
    "import numpy as np\n",
    "\n",
    "dataset = load_boston()\n",
    "print(dataset.DESCR)\n",
    "X=dataset.data\n",
    "Y=dataset.target\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,random_state=0)\n",
    "RidgeModel=Ridge(alpha=100).fit(X_train,Y_train)\n",
    "\n",
    "print(\"The R2 score for Ridge is\",RidgeModel.score(X_test,Y_test))\n",
    "\n",
    "print(\"The number of non-zero coefficients are :\", np.sum(RidgeModel.coef_!=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R2 score for Lasso is 0.11866916175527807\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LassoModel=Lasso(alpha=100).fit(X_train,Y_train)\n",
    "\n",
    "print(\"The R2 score for Lasso is\",LassoModel.score(X_test,Y_test))\n",
    "\n",
    "np.sum(LassoModel.coef_!=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R2 score for Lasso is 0.6350353125168686\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LassoModel=Lasso(alpha=0.001).fit(X_train,Y_train)\n",
    "\n",
    "print(\"The R2 score for Lasso is\",LassoModel.score(X_test,Y_test))\n",
    "\n",
    "np.sum(LassoModel.coef_!=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### D) Comment on your result. In this problem, do all feature seem important in making predictions?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It seems through the R2 score that all features are important but we can't say this for sure as we can see from Lasso model,keeping value of alpha as 100 selects only 2 features for its model while decreasing the value of alpha to as low as 0.001 takes into consideration all the features.There might be other factors involved which would change the R2 score like scaling of the features. Hence, we need to select the an alpha value which gives us the best accuracy which will then tell the useful features. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
